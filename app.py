import os
import tempfile
import streamlit as st
from dotenv import load_dotenv
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.prompts import PromptTemplate
from langchain_community.chat_models import ChatOpenAI
from langchain_core.runnables import chain
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory


# Load environment variables
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")


# Global memory store per session
session_histories = {}


@st.cache_data # Caching to avoid re-processing
def extract_pdf_documents(pdf_files):
    """
    Extracts text from a list of PDF files and returns a list of documents.

    Args:
        pdf_files (list): A list of PDF file objects to extract text from.

    Returns:
        list: A list of documents containing the extracted text.
    """
    docs = []
    for pdf in pdf_files:
        # Save to a temp file first
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(pdf.read())
            tmp_path = tmp_file.name

        loader = PyPDFLoader(tmp_path)
        docs.extend(loader.load())
    return docs


def split_text_into_chunks(docs):
    """
    Splits documents into smaller chunks for better processing.
    
    Args:
        docs (list): A list of documents to be split into chunks.

    Returns:
        list: A list of text chunks.
    """
    splitter = RecursiveCharacterTextSplitter(chunk_size=8000, chunk_overlap=500)
    return splitter.split_documents(docs)


@st.cache_resource # Caching to avoid re-processing
def generate_vector_index(_docs):
    """ Generates a vector index from the provided documents using FAISS.
    
    Args:
        _docs (list): A list of documents to create a vector index from.
    
    Returns:
        None: The function saves the vector index as a retriever.
    """
    embeddings = OpenAIEmbeddings(api_key=openai_api_key, model='text-embedding-3-large')
    vector_store = FAISS.from_documents(_docs, embeddings)
    return vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 5})


def create_qa_chain():
    """ Creates a prompt template using a custom prompt and OpenAI's chat model.
    
    Returns:
        qa_chain: A prompt template that can be used to answer questions based on the context & have the access to the historical chat.
    """
    custom_prompt = """
        You are an assistant for question-answering tasks.
        Use the following pieces of retrieved context to answer the question.
        If you don't know the answer, just say that you don't know.
        Use five sentences maximum and keep the answer concise.

        You also have access to the full chat history, which may help you answer follow-up questions or refer to past messages.
        If the user asks to summarize or list their past questions, extract them from the chat history.
    
        Chat History: {chat_history}
        Question: {question}
        Context: {context}
        Answer:
    """
    llm = ChatOpenAI(model="gpt-4o", api_key=openai_api_key, temperature=0.3)
    prompt_template = PromptTemplate(template=custom_prompt,
                                     input_variables=["context", "question", "chat_history"])
    return prompt_template | llm


def handle_user_query(query, retriever, qa_chain, chat_history):
    """
        Handles user queries by retrieving relevant documents, chat history and invoking the QA chain.

    Args:
        query (str): The user's question.
        retriever: The vector store retriever to fetch relevant documents.
        qa_chain: The QA chain to process the query and context.
        chat_history: The chat history to provide context for the query.

    Returns:
        str: The answer generated by the QA chain.
    """
    docs = retriever.invoke(query)
    context = "\n\n".join([doc.page_content for doc in docs])

    # Format chat history as a string
    formatted_history = ""
    for msg in chat_history:
        formatted_history += f"{msg.type.upper()}: {msg.content}\n"

    return qa_chain.invoke({"question": query, "context": context, "chat_history": formatted_history})


def main():
    """
        Main function to run the Streamlit app for PDF Q&A with memory.
        This function sets up the Streamlit interface, handles file uploads, processes PDFs, and manages chat history.
    """
    st.set_page_config(page_title="PDF Q&A with Memory", page_icon="üìö")
    st.title("üìö Chat with your PDF")

    # --- Upload Section ---
    with st.sidebar:
        uploaded_pdfs = st.file_uploader("Upload PDF files", type=["pdf"], accept_multiple_files=True)

        # Check that the uploaded file is a PDF
        if uploaded_pdfs:
            for pdf in uploaded_pdfs:
                if pdf.type != "application/pdf":
                    st.error(f"{pdf.name} is not a valid PDF file. Please upload a PDF file.")
                    return
        else:
            st.warning("Please upload a PDF file to start.")

        # Clear chat button
        if st.button("üóëÔ∏è Clear Chat"):
            st.session_state.pop("chat_history", None)
            st.session_state.pop("last_uploaded_names", None)
            st.session_state.pop("faiss_index_loaded", None)
            st.success("Chat history and session reset!")

        # Check and reset if new PDFs uploaded
        if uploaded_pdfs:
            current_names = sorted([pdf.name for pdf in uploaded_pdfs])
            previous_names = st.session_state.get("last_uploaded_names")
            
            # If new files uploaded (or first upload)
            if previous_names != current_names:
                st.cache_resource.clear()
                st.session_state["last_uploaded_names"] = current_names
                st.session_state["chat_history"] = InMemoryChatMessageHistory()
                st.session_state.pop("faiss_index_loaded", None)

        # Show debug chat log
        if "chat_history" in st.session_state:
            with st.expander("üîç Chat memory (debug)"):
                for msg in st.session_state["chat_history"].messages:
                    st.write(f"**{msg.type.upper()}**: {msg.content}")

    # --- Main logic ---
    if uploaded_pdfs:
        with st.spinner("Processing PDFs..."):
            # Only load FAISS if not already processed this batch
            if "faiss_index_loaded" not in st.session_state:
                docs = extract_pdf_documents(uploaded_pdfs)
                chunks = split_text_into_chunks(docs)
                retriever = generate_vector_index(chunks)
                st.session_state["faiss_index_loaded"] = retriever
                st.success(f"‚úÖ '{uploaded_pdfs[0].name}' ({len(docs)} pages) processed!")
            else:
                retriever = st.session_state["faiss_index_loaded"]
                docs = []  # to avoid undefined if used later

            # QA Chain
            qa_chain = create_qa_chain()

            # Get or create chat history
            chat_history = st.session_state.get("chat_history", InMemoryChatMessageHistory())
            st.session_state["chat_history"] = chat_history

            @chain
            def history_aware_qa(input):
                """
                    Handles user queries with access to chat history and context.
                """
                question = input["input"]
                chat_history = input["chat_history"]
                return handle_user_query(question, retriever, qa_chain, chat_history)

            qa_with_history = RunnableWithMessageHistory(
                history_aware_qa,
                lambda _: chat_history,
                input_messages_key="input",
                history_messages_key="chat_history",
            )

            # Question box
            st.subheader("üí¨ Ask questions about your PDF")
            with st.chat_message("user"):
                user_query = st.text_input("Your Question:")

            # Answer box
            if user_query:
                with st.spinner("Generating answer..."):
                    result = qa_with_history.invoke(
                            {"input": user_query},
                            config={"configurable": {"session_id": "user-session"}},
                    )
                        
                with st.chat_message("assistant"):
                    st.markdown("Answer:")
                    st.write(result.content)


if __name__ == "__main__":
    main()